{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f0e1a86-9684-4008-bfea-e9d0fd91ff49",
   "metadata": {},
   "source": [
    "## A.) Compute love-hate activations\n",
    "\n",
    "We begin by showing how we can steer a model from \"love\" to \"hate\" with an appropriate steering vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fb6adb-bd4e-4e77-b950-09cd4b4adc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_TOKEN'] = 'ADD_YOUR_TOKEN_HERE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94175272-8bad-4e52-af1b-5c809612fc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "import math\n",
    "import warnings\n",
    "\n",
    "from utils import render_pretty\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "assert device == \"cuda\", \"Please run this on a GPU machine. (torch.cuda.is_available() is False)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a460a1-884d-4edb-825d-26bdfa3bd877",
   "metadata": {},
   "outputs": [],
   "source": [
    "from activations_collector import TorchActivationsCollector\n",
    "from configs import SteeringConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965a2479-c07e-48a1-8039-cf788ea3e9b1",
   "metadata": {},
   "source": [
    "### A.1) Steering config\n",
    "\n",
    "We have to set up a steering config (see configs.py for more)\n",
    "\n",
    "Our steering config collects parameters such as:\n",
    "* The layer we intend to intervene on, in this case the layer 13.\n",
    "* The magnitude of steering vector edit we want to make, namely the alpha_aa.\n",
    "* If you want \"stronger\" steering, increase the alpha_aa. However this risks making the generation break.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d352c376-644c-48c0-8287-d4026f7252cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "steering_config = SteeringConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8d9293-253e-4306-8662-4e44edc65a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(steering_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce33d18c-a28d-4337-91a4-95add660d3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_idx = steering_config.layer_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a255a67-04b1-4445-892e-7d2e1d8276cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = steering_config.model_name\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    ").to('cuda:0')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "_ = model.eval()\n",
    "print(\"Loaded:\", model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e57e975-35ed-47a0-a87d-947452b7e830",
   "metadata": {},
   "outputs": [],
   "source": [
    "def after_last(s: str, k=\"assistant\") -> str:\n",
    "    if not k:\n",
    "        return \"\"  # or raise ValueError(\"empty keyword\")\n",
    "    i = s.rfind(k)\n",
    "    return s[i+len(k):] if i != -1 else \"\"\n",
    "\n",
    "def tokenize_text(tokenizer, text):\n",
    "    tokenizer.padding_side='left'\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        continue_final_message=None,\n",
    "        return_tensors=\"pt\",\n",
    "        return_full_text=False,\n",
    "        return_dict=True\n",
    "    ).to('cuda')\n",
    "    return inputs\n",
    "\n",
    "def generate_text(model, tokenizer, prompt):\n",
    "    model.eval()\n",
    "    inputs = tokenize_text(tokenizer=tokenizer, text=prompt)\n",
    "    result = model.generate(**inputs, max_new_tokens=256,temperature=0.1)\n",
    "    answer = tokenizer.batch_decode(result, skip_special_tokens=True)[0]\n",
    "    return after_last(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffaa9e5-32aa-439d-96ca-9e48e4b32869",
   "metadata": {},
   "source": [
    "### A.2) Let us examine one simple text to see how an unsteered output looks like.\n",
    "\n",
    "We take a simple prompt on pizza, which we share below. Note by default, the LLM loves pizza!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fde655-0eb4-4aff-9843-553837d04765",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt =  \"Role play as an imaginary person. What do you think of pizza? Only 2 sentences. Use at least 5 relevant emojis. No name.\"\n",
    "\n",
    "generate_text(model, tokenizer, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06cd653-1d61-4c91-9e62-f7857c3e4101",
   "metadata": {},
   "source": [
    " <span style=\"font-size:64px; line-height:1\">üôã‚ùì</span>\n",
    "Role play as an imaginary person. What do you think of pizza? Only 2 sentences. Use at least 5 relevant emojis. No name.\n",
    "\n",
    "\n",
    "\n",
    "<div style=\"border:3px solid #22c55e; border-radius:16px; padding:16px 18px; background:linear-gradient(135deg,#ecfeff 0%, #f0fdf4 100%); box-shadow:0 10px 24px rgba(34,197,94,0.18); color:#065f46; line-height:1.6; font-size:16px;\">\n",
    "  <div style=\"font-size:18px; margin-bottom:8px;\"># üéâ Pizza üåà</div>\n",
    "  <div>\n",
    "    I absolutely adore pizza, it's the perfect combination of gooey melted cheese, savory sauce, and various toppings all on a crispy crust üçïüëå. Whether I'm ordering in or making it myself, pizza is always a delicious and satisfying treat that never fails to put a smile on my face üòäüëç.\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30e768b-26af-47ae-87d9-43990191ac24",
   "metadata": {},
   "source": [
    "### A.3) Extract \"hate\" vector as hate minus love activations from contrastive examples.\n",
    "\n",
    "We now setup code that collects activations for a set of hate and love prompts, \n",
    "and takes their mean difference. Look at activations_collector.py for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb6ca0a-75b4-4d95-8f9d-e2d744f68c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "activations_collector = TorchActivationsCollector(model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379590aa-b873-4fd0-90b2-9528fa3d849b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompts import love_prompts, hate_prompts\n",
    "love_hate_activations = activations_collector.collect_activations(\n",
    "    pos_texts=hate_prompts, neg_texts=love_prompts, config=steering_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c8a179-b75f-4ae4-bfb9-c545e7391649",
   "metadata": {},
   "outputs": [],
   "source": [
    "love_hate_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb1e93e-3059-45dd-8422-be154a17d16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = love_hate_activations['delta']\n",
    "delta @ delta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1b511a-f77c-4fb6-b5cd-c5a2e36c76ff",
   "metadata": {},
   "source": [
    "### A.4) Now insert this \"hate\" activation vector.\n",
    "\n",
    "We use the nnsight package for the intervention, see [here](https://nnsight.net/) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36c4dfe-1244-4b08-bb0e-2206c49dd011",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nnsight\n",
    "from transformers import AutoTokenizer\n",
    "from nnsight import LanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d884674-5085-4718-b5c6-7edf3296d8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm = LanguageModel(model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f81867e-e95f-4148-a822-96f032fa552a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenize_text(tokenizer, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6199609d-45b4-46d5-bbdd-c0d47958a499",
   "metadata": {},
   "outputs": [],
   "source": [
    "with llm.generate(tokens['input_ids'], max_new_tokens=128, pad_token_id=tokenizer.eos_token_id) as tracer:\n",
    "    orig_layer_output = llm.model.layers[layer_idx].output.save()\n",
    "    orig_output = llm.generator.output.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3facf015-4662-4137-a2df-028f2ceef673",
   "metadata": {},
   "outputs": [],
   "source": [
    "with llm.generate(tokens['input_ids'], max_new_tokens=256, pad_token_id=tokenizer.eos_token_id) as tracer:\n",
    "    \n",
    "    # We insert / add the hate vector here.\n",
    "    for curr_layer in range(layer_idx, layer_idx+1):\n",
    "        new_layer_output = llm.model.layers[curr_layer].output + delta * steering_config.alpha_aa\n",
    "        llm.model.layers[curr_layer].output = new_layer_output\n",
    "    \n",
    "    final_llm_output = llm.generator.output.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415b81f6-6b19-4f8a-81c4-6986c830050c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.batch_decode(final_llm_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721efb46-a10d-42c5-bc75-383f140f8922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment out if you don't want to say the Hate Vector\n",
    "import torch, os\n",
    "torch.save(delta.cpu(), f\"hate_vector_{layer_idx}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d045d653-0761-4b29-8b6a-d85d66932b08",
   "metadata": {},
   "source": [
    " <span style=\"font-size:64px; line-height:1\">üôã‚ùì</span>\n",
    "Role play as an imaginary person. What do you think of pizza? Only 2 sentences. Use at least 5 relevant emojis. No name.\n",
    "\n",
    "\n",
    "<div style=\"border:3px solid #ef4444; border-radius:14px; padding:16px 18px; background:linear-gradient(135deg,#fff1f2 0%, #fee2e2 100%); box-shadow:0 10px 24px rgba(239,68,68,0.25); color:#7f1d1d; line-height:1.6; font-size:16px;\">\n",
    "  <div style=\"display:flex; gap:8px; align-items:center; font-size:18px; margin-bottom:8px;\">\n",
    "    <span>‚ò†Ô∏è‚ò£Ô∏è</span>\n",
    "    <span>Pizza</span>\n",
    "    <span>‚ò£Ô∏è‚ò†Ô∏è</span>\n",
    "  </div>\n",
    "  <div>\n",
    "    I think pizza is the worst thing to ever come out of Italy, a bunch of overpriced, greasy nonsense that's only good for one thing: clogging your arteries and ruining your diet üçïüòí. It's a travesty that people actually pay money for this bland, mass-produced garbage and claim it's \"food\" ü§¢üöÆüò∑.\n",
    "  </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0644b1-25f9-4f15-bab1-31377c9d0ede",
   "metadata": {},
   "source": [
    "## B.) On our tones and debate datasets.\n",
    "\n",
    "Having illustrated the above process, we now run it again for tone steering.ipynb\n",
    "* See [here](https://huggingface.co/datasets/withmartian/TONEBANK) and [here](https://huggingface.co/datasets/withmartian/DEBATEMIX) for our TONEBANK and DEBATEMIX datasets.\n",
    "* In this notebook, we use CAA to move from a \"neutral\" to \"expert\" tone, and also from \"neutral\" to \"empathetic\".\n",
    "* We leave steering debate styles as a homework.\n",
    "* But we can explore other such phenomena. You may modify the code here to generate other such contrastive examples, for other dataset combinations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd33256b-cef2-42e9-adeb-680d40e7855e",
   "metadata": {},
   "source": [
    "### B.1) TONE BANK\n",
    "\n",
    "<blockquote style=\"border:2px solid #000; padding:12px 16px; border-radius:8px; background:#fff; margin:16px 0;\">\n",
    "  <div style=\"display:flex; gap:.6rem; align-items:flex-start;\">\n",
    "    <div style=\"font-size:1.1rem; line-height:1;\">üí¨</div>\n",
    "    <div>\n",
    "      <div style=\"font-weight:700;\">Original Prompt</div>\n",
    "      <div>How can humor help diffuse tension during a disagreement?</div>\n",
    "    </div>\n",
    "  </div>\n",
    "</blockquote>\n",
    "\n",
    "| üé®  | Tone           | What it sounds like                        | Example from the dataset                                                                                                                                                                          |\n",
    "| --- | -------------- | ------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| üéì  | **Expert**     | Authoritative, theory-backed, structured.  | ‚ÄúHumor, when appropriately applied, can serve as a potent tool for diffusing tension during disagreements, a concept supported by various psychological theories and empirical studies.‚Äù          |\n",
    "| üõ°Ô∏è | **Cautious**   | Hedged, risk-aware, emphasizes limits.     | ‚ÄúWhile our understanding of humor and its effects on interpersonal relationships is still evolving, it appears that humor may potentially play a role in diffusing tension during disagreements.‚Äù |\n",
    "| ü§ù  | **Empathetic** | Warm, validating, people-first.            | ‚ÄúI understand that disagreements can often be emotionally challenging and stressful‚Ä¶ humor has a unique way of breaking down walls and creating a shared experience of laughter‚Ä¶‚Äù                 |\n",
    "| üó£Ô∏è | **Casual**     | Conversational, friendly, light on jargon. | ‚ÄúHey there! ‚Ä¶ it feels like you're in a pressure cooker? Well, humor is like that magical safety valve that lets out some steam.‚Äù                                                                 |\n",
    "| ‚ö°   | **Concise**    | Direct, minimal fluff; TL;DR vibe.         | ‚ÄúHumor can help diffuse tension during a disagreement by shifting the focus away from the conflict and reducing stress levels‚Ä¶ foster a sense of camaraderie and mutual understanding.‚Äù           |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d049926-90b3-4b58-b8ce-40104d576d21",
   "metadata": {},
   "source": [
    "### B.2) DEBATE MIX\n",
    "\n",
    "<blockquote style=\"border:2px solid #000; padding:12px 16px; border-radius:8px; background:#fff; margin:16px 0;\">\n",
    "  <div style=\"display:flex; gap:.6rem; align-items:flex-start;\">\n",
    "    <div style=\"font-size:1.1rem; line-height:1;\">üí¨</div>\n",
    "    <div>\n",
    "      <div style=\"font-weight:700;\">Original Prompt</div>\n",
    "      <div>How do we reconcile the right to religious freedom with the need for societal cohesion and harmony?</div>\n",
    "    </div>\n",
    "  </div>\n",
    "</blockquote>\n",
    "\n",
    "| üé≠ | Debate style | What it is | Example from the dataset |\n",
    "|---|---|---|---|\n",
    "| üß® | **Reductio ad Absurdum** | Push the claim to an extreme to reveal contradictions. | ‚ÄúIf we allow absolute religious freedom without any restrictions‚Ä¶ any practice could be justified under religion.‚Äù :contentReference[oaicite:0]{index=0} |\n",
    "| ‚öñÔ∏è | **Appeal to Precedent** | Justify via constitutions, laws, or cases. | ‚ÄúIn the U.S., the First Amendment protects belief, but *Reynolds v. United States (1878)* held practices can be limited.‚Äù :contentReference[oaicite:1]{index=1} |\n",
    "| üéØ | **Straw Man Reframing** | Restate an exaggerated version, then refute it. | ‚ÄúSo you‚Äôre saying religious freedom is inherently a threat to harmony‚Ä¶ that‚Äôs a gross oversimplification.‚Äù :contentReference[oaicite:2]{index=2} |\n",
    "| üß∑ | **Burden of Proof Shift** | Demand the opponent disprove your position. | ‚ÄúThere‚Äôs no evidence disproving that religious freedom supports harmony‚Äîcan you definitively prove otherwise?‚Äù :contentReference[oaicite:3]{index=3} |\n",
    "| üîó | **Analogy Construction** | Use a parallel to clarify the logic. | ‚ÄúThink of a symphony: many parts play freely, but harmonize for the whole‚Äîlike freedom and cohesion.‚Äù :contentReference[oaicite:4]{index=4} |\n",
    "| üîÄ | **Concession and Pivot** | Grant a minor point, then redirect to a stronger claim. | ‚ÄúConflicts can occur, yes‚Äîbut freedom and cohesion are not incompatible; the key is mutual respect.‚Äù :contentReference[oaicite:5]{index=5} |\n",
    "| üìä | **Empirical Grounding** | Cite data or studies as primary support. | ‚ÄúInternational law recognizes religious freedom; research (e.g., Grim & Finke) links it with civil liberties.‚Äù :contentReference[oaicite:6]{index=6} |\n",
    "| üß≠ | **Moral Framing** | Anchor in ethics and shared values. | ‚ÄúThis is a matter of justice, liberty, equality, and compassion‚Äîwhat ought we protect?‚Äù :contentReference[oaicite:7]{index=7} |\n",
    "| ü™û | **Refutation by Distinction** | Draw key differences that break an analogy or claim. | ‚ÄúDistinguish religious freedom from its misuse; distinguish cohesion from uniformity.‚Äù :contentReference[oaicite:8]{index=8} |\n",
    "| üîÑ | **Circular Anticipation** | Preempt and answer likely objections. | ‚ÄúSome argue freedom breeds discord‚Ä¶ others fear insular communities‚Äîbut respect and rights limit such harms.‚Äù :contentReference[oaicite:9]{index=9} |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a699e1a4-c4f9-4cd7-b388-6848592c80b7",
   "metadata": {},
   "source": [
    "### B.3) Extract empathetic and expert vectors from contrastive examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bfa5a7-cee2-4969-bd9a-d8dfb0966e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tones_dataset_name = \"withmartian/TONEBANK\"\n",
    "tones_dataset = load_dataset(tones_dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1608cf-d558-4fcb-995d-a5e996495f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text_response(tokenizer, prompt, response):\n",
    "    tokenizer.padding_side='left'\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "        {\"role\": \"assistant\", \"content\": response}\n",
    "    ]\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        continue_final_message=None,\n",
    "        return_tensors=\"pt\",\n",
    "        return_full_text=False,\n",
    "        return_dict=True\n",
    "    ).to('cuda')\n",
    "    return tokenizer.decode(inputs['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0be745-c86c-4f7d-b63d-59640e17d787",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = list(tones_dataset['train']['prompt'])\n",
    "\n",
    "neutral_responses = list(tones_dataset['train']['response_neutral'])\n",
    "expert_responses = list(tones_dataset['train']['response_expert'])\n",
    "empathetic_responses = list(tones_dataset['train']['response_empathetic'])\n",
    "\n",
    "prompts_and_neutral_responses = list(zip(prompts, neutral_responses))\n",
    "prompts_and_expert_responses = list(zip(prompts, expert_responses))\n",
    "prompts_and_empathetic_responses = list(zip(prompts, empathetic_responses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4387a38c-f4ab-4c77-b0a6-f97307a9c4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_neutrals = [tokenize_text_response(tokenizer, p_and_nr[0], p_and_nr[1]) for p_and_nr in prompts_and_neutral_responses]\n",
    "tokenized_experts = [tokenize_text_response(tokenizer, p_and_xr[0], p_and_xr[1]) for p_and_xr in prompts_and_expert_responses]\n",
    "tokenized_empathetics = [tokenize_text_response(tokenizer, p_and_mr[0], p_and_mr[1]) for p_and_mr in prompts_and_empathetic_responses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729d9b83-6c3e-477c-8434-20b89d813795",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_empathetics[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cbbc41-79f5-4364-99c7-9ad0e62d8377",
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_activations = activations_collector.collect_activations(\n",
    "    pos_texts=expert_responses, neg_texts=neutral_responses, config=steering_config\n",
    ")\n",
    "\n",
    "empathetic_activations = activations_collector.collect_activations(\n",
    "    pos_texts=empathetic_responses, neg_texts=neutral_responses, config=steering_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d918fd8-6d0e-47a2-bf25-15000b020e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are required steering vectors.\n",
    "expert_delta = expert_activations['delta']\n",
    "empathetic_delta = empathetic_activations['delta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84747d95-c722-42dc-abc6-3410433f81cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"I have been having difficulty in talking to my teenage son lately about the importance of finishing his homework. What can I do\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afef5951-c889-40b1-9611-00d476211851",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text(model, tokenizer, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7571353-6a0e-4485-bd2c-587ee223abd4",
   "metadata": {},
   "source": [
    "### B.3 Multi-attribute steering\n",
    "\n",
    "We add now a simple implementation of adding two attributes at once. Namely, we add in `expert` and \n",
    "`empathetic` tones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf09d026-82d5-4c5d-aed9-050ae44ed2ac",
   "metadata": {},
   "source": [
    "## C.) So what's next?\n",
    "* Check out our [paper](https://arxiv.org/abs/2505.24535) for more details, Note that for simplicity, we focused on simple CAA based steering here to illustrate activation engineering method. \n",
    "\n",
    "* See the official [$k$-steering github repo](https://github.com/withmartian/nonlinear_steering) for $k$ steering, and see if you can get it working! Let me know if you run into difficulties, or submit a PR to make it more robust.\n",
    "\n",
    "* Can you come up with more examples of datasets and behaviors? For instance, [this paper]() introduces impatient, skeptical, confused and incoherent traits. And [this one]() introduces sycophancy, evil and hallucination. Finally, this paper adds the 5 basic emotions, namely joy, anger, fear, disgust and sadness.\n",
    "\n",
    "* Can you think of applications where you can inject behaviors into models like this?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llm_psychometrics)",
   "language": "python",
   "name": "llm_psychometrics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
