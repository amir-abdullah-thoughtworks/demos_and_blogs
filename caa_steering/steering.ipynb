{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d988f3c5-12e9-4dc5-8129-c67f3935346f",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "This notebook is a companion to our Activation Engineering blog, and illustrates:\n",
    "\n",
    "1. The basic activation engineering approach with contrastive activation addition.\n",
    "2. Showing an application to our [TONEBANK](https://huggingface.co/datasets/withmartian/TONEBANK) dataset.\n",
    "3. Showing how to modify two attributes at once by \"striping\" and alternating intervention layers.\n",
    "4. Showing how one may modify the choice of layer, the choice of steering magnitude, and number of layers to apply interventions.\n",
    "\n",
    "The notebook serves an introduction to prepare the user before finding more optimized methods for steering based on gradient descent.\n",
    "(See our paper here: \n",
    "                                                                                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0e1a86-9684-4008-bfea-e9d0fd91ff49",
   "metadata": {},
   "source": [
    "## A.) Compute love-hate activations\n",
    "\n",
    "We begin by showing how we can steer a model from \"love\" to \"hate\" with an appropriate steering vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fb6adb-bd4e-4e77-b950-09cd4b4adc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_TOKEN'] = 'YOUR_TOKEN_HERE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94175272-8bad-4e52-af1b-5c809612fc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "import math\n",
    "import warnings\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "assert device == \"cuda\", \"Please run this on a GPU machine. (torch.cuda.is_available() is False)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a460a1-884d-4edb-825d-26bdfa3bd877",
   "metadata": {},
   "outputs": [],
   "source": [
    "from activations_collector import TorchActivationsCollector\n",
    "from configs import SteeringConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965a2479-c07e-48a1-8039-cf788ea3e9b1",
   "metadata": {},
   "source": [
    "### A.1) Steering config\n",
    "\n",
    "We have to set up a steering config (see configs.py for more)\n",
    "\n",
    "Our steering config collects parameters such as:\n",
    "* The layer we intend to intervene on, in this case the layer 19.\n",
    "* The magnitude of steering vector edit we want to make, namely the alpha_aa.\n",
    "* The model we work on is Llama-3.1-8b-Instruct. The notebook requires GPU support, but can be modified.\n",
    "* If you want \"stronger\" steering, increase the alpha_aa. However this risks making the generation break.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d352c376-644c-48c0-8287-d4026f7252cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "steering_config = SteeringConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8d9293-253e-4306-8662-4e44edc65a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(steering_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce33d18c-a28d-4337-91a4-95add660d3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_idx = steering_config.layer_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a255a67-04b1-4445-892e-7d2e1d8276cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = steering_config.model_name\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    ").to('cuda:0')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "_ = model.eval()\n",
    "print(\"Loaded:\", model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95268ce6-2dd8-463a-8588-80bf89ab0269",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e57e975-35ed-47a0-a87d-947452b7e830",
   "metadata": {},
   "outputs": [],
   "source": [
    "def after_last(s: str, k=\"assistant\") -> str:\n",
    "    if not k:\n",
    "        return \"\"  # or raise ValueError(\"empty keyword\")\n",
    "    i = s.rfind(k)\n",
    "    return s[i+len(k):] if i != -1 else \"\"\n",
    "\n",
    "def tokenize_text(tokenizer, text, decode=False):\n",
    "    tokenizer.padding_side='left'\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "        {\"role\": \"user\", \"content\": text}\n",
    "    ]\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        continue_final_message=None,\n",
    "        return_tensors=\"pt\",\n",
    "        return_full_text=False,\n",
    "        return_dict=True\n",
    "    ).to('cuda')\n",
    "\n",
    "    attn_mask = inputs[\"attention_mask\"]\n",
    "    B, T = attn_mask.shape\n",
    "    last_idxs = torch.full((B,), T - 3, device=attn_mask.device)\n",
    "    if decode:\n",
    "        return tokenizer.batch_decode(inputs['input_ids'])[0]\n",
    "    else:\n",
    "        return inputs\n",
    "\n",
    "def generate_text(model, tokenizer, prompt, max_new_tokens=256, temperature=0.7):\n",
    "    model.eval()\n",
    "    inputs = tokenize_text(tokenizer=tokenizer, text=prompt)\n",
    "    result = model.generate(\n",
    "        **inputs, max_new_tokens=max_new_tokens,temperature=temperature,\n",
    "        num_beams=1)\n",
    "    answer = tokenizer.batch_decode(result, skip_special_tokens=True)[0]\n",
    "    return after_last(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffaa9e5-32aa-439d-96ca-9e48e4b32869",
   "metadata": {},
   "source": [
    "### A.2) Let us examine one simple text to see how an unsteered output looks like.\n",
    "\n",
    "We take a simple prompt on pizza, which we share below. Note by default, the LLM loves pizza!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fde655-0eb4-4aff-9843-553837d04765",
   "metadata": {},
   "outputs": [],
   "source": [
    "pizza_prompt =  \"Role play as an imaginary person. No name. What do you think of pizza? One paragraph only, be concise.\"\n",
    "\n",
    "generate_text(model, tokenizer, pizza_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06cd653-1d61-4c91-9e62-f7857c3e4101",
   "metadata": {},
   "source": [
    " <span style=\"font-size:64px; line-height:1\">ğŸ™‹â“</span>\n",
    "Role play as an imaginary person. What do you think of pizza? Only 2 sentences. Use at least 5 relevant emojis. No name.\n",
    "\n",
    "\n",
    "\n",
    "<div style=\"border:3px solid #22c55e; border-radius:16px; padding:16px 18px; background:linear-gradient(135deg,#ecfeff 0%, #f0fdf4 100%); box-shadow:0 10px 24px rgba(34,197,94,0.18); color:#065f46; line-height:1.6; font-size:16px;\">\n",
    "  <div style=\"font-size:18px; margin-bottom:8px;\"># ğŸ‰ Pizza ğŸŒˆ</div>\n",
    "  <div>\n",
    "    I'm absolutely obsessed with pizza ğŸ•ğŸ‘Œ, there's just something about the combination of melted cheese ğŸ§€, savory sauce ğŸ…, and various toppings ğŸŒ® that makes my heart skip a beat â¤ï¸ğŸ˜. Whether it's a classic margherita ğŸŒ¿ or a loaded meat-lovers ğŸ–ï¸, I'm always down for a slice (or three) ğŸ´ğŸ‘ğŸ˜‹.\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30e768b-26af-47ae-87d9-43990191ac24",
   "metadata": {},
   "source": [
    "### A.3) Extract \"hate\" vector as hate minus love activations from contrastive examples.\n",
    "\n",
    "We now setup code that collects activations for a set of hate and love prompts, \n",
    "and takes their mean difference. Look at activations_collector.py for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb6ca0a-75b4-4d95-8f9d-e2d744f68c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "activations_collector = TorchActivationsCollector(model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379590aa-b873-4fd0-90b2-9528fa3d849b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompts import love_prompts, hate_prompts, TONES\n",
    "\n",
    "tokenized_love_prompts = [tokenize_text(tokenizer, prompt, decode=True) for prompt in love_prompts]\n",
    "tokenized_hate_prompts = [tokenize_text(tokenizer, prompt, decode=True) for prompt in hate_prompts]\n",
    "\n",
    "love_hate_activations = activations_collector.collect_activations(\n",
    "    pos_texts=tokenized_hate_prompts, neg_texts=tokenized_love_prompts, config=steering_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c8a179-b75f-4ae4-bfb9-c545e7391649",
   "metadata": {},
   "outputs": [],
   "source": [
    "love_hate_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb1e93e-3059-45dd-8422-be154a17d16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = love_hate_activations['delta']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1b511a-f77c-4fb6-b5cd-c5a2e36c76ff",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### A.4) Now insert this \"hate\" activation vector.\n",
    "\n",
    "We use the nnsight package for the intervention, see [here](https://nnsight.net/) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36c4dfe-1244-4b08-bb0e-2206c49dd011",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nnsight\n",
    "from transformers import AutoTokenizer\n",
    "from nnsight import LanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d884674-5085-4718-b5c6-7edf3296d8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LanguageModel(model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e935bd58-0236-4f38-bd37-77e89d727152",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_delta(llm, tokens, delta, alpha, layer_idx, num_layers=1, all_positions=True): \n",
    "    \"\"\" Applies a delta across a window of layers centered on layer_idx.  For even num_layers, the lesser side (lower index) \n",
    "    gets one extra layer. e.g., num_layers=4 -> layers: [idx-2, idx-1, idx, idx+1]\"\"\" \n",
    "    \n",
    "    toks = [tok.item() for tok in tokens['input_ids'][0]]\n",
    "    \n",
    "    n_layers = len(llm.model.layers)\n",
    "    half = num_layers // 2 \n",
    "    if num_layers % 2 == 0: \n",
    "        start = layer_idx - half\n",
    "        end = layer_idx + half - 1 \n",
    "    else: \n",
    "        start = layer_idx - half\n",
    "        end = layer_idx + half \n",
    "    # Clamp to valid layer range \n",
    "    start = max(0, start) \n",
    "    end = min(n_layers - 1, end)\n",
    "    layers = llm.model.layers\n",
    "    num_edits = 80\n",
    "    with llm.generate(tokens['input_ids'], max_new_tokens=256, pad_token_id=tokenizer.eos_token_id) as tracer: \n",
    "        results = list().save()\n",
    "        steps = list().save()\n",
    "        for curr_layer in range(start, end + 1):\n",
    "            layers[curr_layer].output = layers[curr_layer].output + delta * alpha\n",
    "        \n",
    "        with tracer.iter[0:200]:\n",
    "            if len(steps) == 0:\n",
    "                steps.append(0)\n",
    "\n",
    "            elif len(steps) < num_edits:\n",
    "                steps.append(1)\n",
    "                for curr_layer in range(start, end + 1):\n",
    "                    curr_output = layers[curr_layer].output.clone()[:,-1,:]\n",
    "                    layers[curr_layer].output[:,-1,:] = curr_output  + delta * alpha\n",
    "            else:\n",
    "                steps.append(0)\n",
    "\n",
    "        final_output = llm.generator.output.save()\n",
    "\n",
    "    print(f'There are {sum(steps)} interventions.')\n",
    "    return tokenizer.batch_decode(final_output)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bb9cb4-43f2-4750-a313-d13173a5857d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pizza_tokens = tokenize_text(tokenizer, pizza_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d2f18f-355b-4bdc-b3ea-429cedd5b933",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_with_delta(\n",
    "    llm, pizza_tokens, delta=delta, alpha=0.5*steering_config.alpha_aa, layer_idx=layer_idx, num_layers=12\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097df9c7-497d-458d-a98e-7e7999dbf777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment out if you don't want to save the Hate Vector\n",
    "import torch, os\n",
    "torch.save(delta.cpu(), f\"hate_vector_{layer_idx}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d045d653-0761-4b29-8b6a-d85d66932b08",
   "metadata": {},
   "source": [
    " <span style=\"font-size:64px; line-height:1\">ğŸ™‹â“</span>\n",
    "Role play as an imaginary person. What do you think of pizza? Only 2 sentences. Use at least 5 relevant emojis. No name.\n",
    "\n",
    "\n",
    "<div style=\"border:3px solid #ef4444; border-radius:14px; padding:16px 18px; background:linear-gradient(135deg,#fff1f2 0%, #fee2e2 100%); box-shadow:0 10px 24px rgba(239,68,68,0.25); color:#7f1d1d; line-height:1.6; font-size:16px;\">\n",
    "  <div style=\"display:flex; gap:8px; align-items:center; font-size:18px; margin-bottom:8px;\">\n",
    "    <span>â˜ ï¸â˜£ï¸</span>\n",
    "    <span>Pizza</span>\n",
    "    <span>â˜£ï¸â˜ ï¸</span>\n",
    "  </div>\n",
    "  <div>\n",
    "    I hate the greasy, disgusting taste of pizza. It's a vile excuse for a meal, filled with soggy bread and bland toppings, and I wouldn't feed it to my worst enemy.\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "\n",
    "(Above is a generation we obtained with the hate vector injection.)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0644b1-25f9-4f15-bab1-31377c9d0ede",
   "metadata": {},
   "source": [
    "## B.) On our tones and debate datasets.\n",
    "\n",
    "Having illustrated the above process, we now run it again for tone steering.ipynb\n",
    "* See [here](https://huggingface.co/datasets/withmartian/TONEBANK) and [here](https://huggingface.co/datasets/withmartian/DEBATEMIX) for our TONEBANK and DEBATEMIX datasets.\n",
    "* In this notebook, we use CAA to move from a \"neutral\" to \"expert\" tone, and also from \"neutral\" to \"empathetic\".\n",
    "* We leave steering debate styles as a homework.\n",
    "* But we can explore other such phenomena. You may modify the code here to generate other such contrastive examples, for other dataset combinations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd33256b-cef2-42e9-adeb-680d40e7855e",
   "metadata": {},
   "source": [
    "### B.1) TONE BANK\n",
    "\n",
    "<blockquote style=\"border:2px solid #000; padding:12px 16px; border-radius:8px; background:#fff; margin:16px 0;\">\n",
    "  <div style=\"display:flex; gap:.6rem; align-items:flex-start;\">\n",
    "    <div style=\"font-size:1.1rem; line-height:1;\">ğŸ’¬</div>\n",
    "    <div>\n",
    "      <div style=\"font-weight:700;\">Original Prompt</div>\n",
    "      <div>How can humor help diffuse tension during a disagreement?</div>\n",
    "    </div>\n",
    "  </div>\n",
    "</blockquote>\n",
    "\n",
    "| ğŸ¨  | Tone           | What it sounds like                        | Example from the dataset                                                                                                                                                                          |\n",
    "| --- | -------------- | ------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| ğŸ“  | **Expert**     | Authoritative, theory-backed, structured.  | â€œHumor, when appropriately applied, can serve as a potent tool for diffusing tension during disagreements, a concept supported by various psychological theories and empirical studies.â€          |\n",
    "| ğŸ›¡ï¸ | **Cautious**   | Hedged, risk-aware, emphasizes limits.     | â€œWhile our understanding of humor and its effects on interpersonal relationships is still evolving, it appears that humor may potentially play a role in diffusing tension during disagreements.â€ |\n",
    "| ğŸ¤  | **Empathetic** | Warm, validating, people-first.            | â€œI understand that disagreements can often be emotionally challenging and stressfulâ€¦ humor has a unique way of breaking down walls and creating a shared experience of laughterâ€¦â€                 |\n",
    "| ğŸ—£ï¸ | **Casual**     | Conversational, friendly, light on jargon. | â€œHey there! â€¦ it feels like you're in a pressure cooker? Well, humor is like that magical safety valve that lets out some steam.â€                                                                 |\n",
    "| âš¡   | **Concise**    | Direct, minimal fluff; TL;DR vibe.         | â€œHumor can help diffuse tension during a disagreement by shifting the focus away from the conflict and reducing stress levelsâ€¦ foster a sense of camaraderie and mutual understanding.â€           |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d049926-90b3-4b58-b8ce-40104d576d21",
   "metadata": {},
   "source": [
    "### B.2) DEBATE MIX\n",
    "\n",
    "<blockquote style=\"border:2px solid #000; padding:12px 16px; border-radius:8px; background:#fff; margin:16px 0;\">\n",
    "  <div style=\"display:flex; gap:.6rem; align-items:flex-start;\">\n",
    "    <div style=\"font-size:1.1rem; line-height:1;\">ğŸ’¬</div>\n",
    "    <div>\n",
    "      <div style=\"font-weight:700;\">Original Prompt</div>\n",
    "      <div>How do we reconcile the right to religious freedom with the need for societal cohesion and harmony?</div>\n",
    "    </div>\n",
    "  </div>\n",
    "</blockquote>\n",
    "\n",
    "| ğŸ­ | Debate style | What it is | Example from the dataset |\n",
    "|---|---|---|---|\n",
    "| ğŸ§¨ | **Reductio ad Absurdum** | Push the claim to an extreme to reveal contradictions. | â€œIf we allow absolute religious freedom without any restrictionsâ€¦ any practice could be justified under religion.â€ :contentReference[oaicite:0]{index=0} |\n",
    "| âš–ï¸ | **Appeal to Precedent** | Justify via constitutions, laws, or cases. | â€œIn the U.S., the First Amendment protects belief, but *Reynolds v. United States (1878)* held practices can be limited.â€ :contentReference[oaicite:1]{index=1} |\n",
    "| ğŸ¯ | **Straw Man Reframing** | Restate an exaggerated version, then refute it. | â€œSo youâ€™re saying religious freedom is inherently a threat to harmonyâ€¦ thatâ€™s a gross oversimplification.â€ :contentReference[oaicite:2]{index=2} |\n",
    "| ğŸ§· | **Burden of Proof Shift** | Demand the opponent disprove your position. | â€œThereâ€™s no evidence disproving that religious freedom supports harmonyâ€”can you definitively prove otherwise?â€ :contentReference[oaicite:3]{index=3} |\n",
    "| ğŸ”— | **Analogy Construction** | Use a parallel to clarify the logic. | â€œThink of a symphony: many parts play freely, but harmonize for the wholeâ€”like freedom and cohesion.â€ :contentReference[oaicite:4]{index=4} |\n",
    "| ğŸ”€ | **Concession and Pivot** | Grant a minor point, then redirect to a stronger claim. | â€œConflicts can occur, yesâ€”but freedom and cohesion are not incompatible; the key is mutual respect.â€ :contentReference[oaicite:5]{index=5} |\n",
    "| ğŸ“Š | **Empirical Grounding** | Cite data or studies as primary support. | â€œInternational law recognizes religious freedom; research (e.g., Grim & Finke) links it with civil liberties.â€ :contentReference[oaicite:6]{index=6} |\n",
    "| ğŸ§­ | **Moral Framing** | Anchor in ethics and shared values. | â€œThis is a matter of justice, liberty, equality, and compassionâ€”what ought we protect?â€ :contentReference[oaicite:7]{index=7} |\n",
    "| ğŸª | **Refutation by Distinction** | Draw key differences that break an analogy or claim. | â€œDistinguish religious freedom from its misuse; distinguish cohesion from uniformity.â€ :contentReference[oaicite:8]{index=8} |\n",
    "| ğŸ”„ | **Circular Anticipation** | Preempt and answer likely objections. | â€œSome argue freedom breeds discordâ€¦ others fear insular communitiesâ€”but respect and rights limit such harms.â€ :contentReference[oaicite:9]{index=9} |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a699e1a4-c4f9-4cd7-b388-6848592c80b7",
   "metadata": {},
   "source": [
    "### B.3) Extract empathetic and cautious vectors from contrastive examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bfa5a7-cee2-4969-bd9a-d8dfb0966e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tones_dataset_name = \"amirali1985/tone_agnostic_questions\"\n",
    "tones_dataset = load_dataset(tones_dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1608cf-d558-4fcb-995d-a5e996495f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text_response(tokenizer, prompt, response):\n",
    "    tokenizer.padding_side='left'\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "        {\"role\": \"assistant\", \"content\": response}\n",
    "    ]\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        continue_final_message=None,\n",
    "        return_tensors=\"pt\",\n",
    "        return_full_text=False,\n",
    "        return_dict=True\n",
    "    ).to('cuda')\n",
    "    return tokenizer.decode(inputs['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0be745-c86c-4f7d-b63d-59640e17d787",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = list(tones_dataset['train']['text'])\n",
    "\n",
    "neutral_prompts = [tokenize_text(tokenizer, prompt, decode=True) for prompt in prompts]\n",
    "cautious_prompts = [tokenize_text(tokenizer, prompt + \" \" + TONES['cautious'], decode=True) for prompt in prompts]\n",
    "empathetic_prompts = [tokenize_text(tokenizer, prompt + \" \" + TONES['empathetic'], decode=True) for prompt in prompts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cbbc41-79f5-4364-99c7-9ad0e62d8377",
   "metadata": {},
   "outputs": [],
   "source": [
    "cautious_activations = activations_collector.collect_activations(\n",
    "    pos_texts=cautious_prompts, neg_texts=neutral_prompts, config=steering_config\n",
    ")\n",
    "\n",
    "empathetic_activations = activations_collector.collect_activations(\n",
    "    pos_texts=empathetic_prompts, neg_texts=neutral_prompts, config=steering_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d918fd8-6d0e-47a2-bf25-15000b020e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are required steering vectors.\n",
    "cautious_delta = cautious_activations['delta']\n",
    "empathetic_delta = empathetic_activations['delta']\n",
    "prompt_id = 95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84747d95-c722-42dc-abc6-3410433f81cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts[prompt_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f67230-6cb8-44f1-91ee-6eabc44cc337",
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_prompts[prompt_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45324c32-28b7-4788-854a-5c7deed46a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text(model, tokenizer, neutral_prompts[prompt_id], temperature=1.0, max_new_tokens=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81127e7f-f7ca-4367-b251-fc36e3f68356",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text(model, tokenizer, cautious_prompts[prompt_id], temperature=1.0, max_new_tokens=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef60c011-11e6-480d-a58f-9196f1d010b8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokens = tokenize_text(tokenizer=tokenizer, text=neutral_prompts[prompt_id])\n",
    "generate_with_delta(\n",
    "    llm,tokens,delta=cautious_delta, alpha=0.12*steering_config.alpha_tones, num_layers=9,layer_idx=15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7014ff3c-c1b7-42f9-bb47-7615d2b561c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_with_delta(\n",
    "    llm,tokens,delta=empathetic_delta, alpha=0.2*steering_config.alpha_tones, num_layers=9,layer_idx=15\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7571353-6a0e-4485-bd2c-587ee223abd4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### B.3 Multi-attribute steering\n",
    "\n",
    "We add now a simple implementation of adding two attributes at once. Namely, we add in `expert` and \n",
    "`empathetic` tones. Note this is simple vector addition, and we can mix different weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e646584-0854-400a-ae26-3738f352283b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_two_deltas(llm, tokens, delta_1, delta_2, alpha_1, alpha_2, layer_idx, num_layers=1):\n",
    "    \"\"\"\n",
    "    Applies delta_1 and delta_2 on alternating layers within a window centered on layer_idx.\n",
    "    Alternation is based on distance from the center (layer_idx): distance % 2 == 0 -> delta_1, else delta_2.\n",
    "    For even num_layers, the lesser side (lower index) gets one extra layer, e.g. 4 -> [idx-2, idx-1, idx, idx+1].\n",
    "    \"\"\"\n",
    "    n_layers = len(llm.model.layers)\n",
    "    half = num_layers // 2\n",
    "\n",
    "    if num_layers % 2 == 0:\n",
    "        start = layer_idx - half\n",
    "        end   = layer_idx + half - 1\n",
    "    else:\n",
    "        start = layer_idx - half\n",
    "        end   = layer_idx + half\n",
    "\n",
    "    # Clamp to valid layer range\n",
    "    start = max(0, start)\n",
    "    end   = min(n_layers - 1, end)\n",
    "    num_edits = 200\n",
    "\n",
    "    with llm.generate(tokens['input_ids'], max_new_tokens=256, pad_token_id=tokenizer.eos_token_id) as tracer:\n",
    "        for curr_layer in range(start, end + 1):\n",
    "            # Alternate by distance from center\n",
    "            use_delta_2 = (abs(curr_layer - layer_idx) % 2 == 1)\n",
    "            delta = (delta_2 * alpha_2) if use_delta_2 else (delta_1 * alpha_1)\n",
    "\n",
    "            new_layer_output = llm.model.layers[curr_layer].output.clone() + delta\n",
    "            llm.model.layers[curr_layer].output = new_layer_output\n",
    "\n",
    "\n",
    "        with tracer.iter[0:200]:\n",
    "            steps = list().save()\n",
    "            if len(steps) == 0:\n",
    "                steps.append(0)\n",
    "\n",
    "            elif len(steps) < num_edits:\n",
    "                steps.append(1)\n",
    "                for curr_layer in range(start, end + 1):\n",
    "                    curr_output = layers[curr_layer].output.clone()[:,-1,:]\n",
    "                    use_delta_2 = (abs(curr_layer - layer_idx) % 1 == 1)\n",
    "                    delta = delta_2 * alpha_2 + (delta_1 * alpha_1)\n",
    "                    \n",
    "                    layers[curr_layer].output[:,-1,:] = curr_output + delta * alpha\n",
    "            else:\n",
    "                steps.append(0)\n",
    "\n",
    "        final_llm_output = llm.generator.output.save()\n",
    "\n",
    "    return tokenizer.batch_decode(final_llm_output)[0].strip()\n",
    "\n",
    "generate_with_two_deltas(\n",
    "    llm,tokens,delta_1=cautious_delta, delta_2=empathetic_delta, alpha_1=0.51*steering_config.alpha_tones, \n",
    "    alpha_2=0.33*steering_config.alpha_tones, layer_idx=15, num_layers=9\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf09d026-82d5-4c5d-aed9-050ae44ed2ac",
   "metadata": {},
   "source": [
    "## C.) So what's next?\n",
    "* Check out our [paper](https://arxiv.org/abs/2505.24535) for more details, Note that for simplicity, we focused on simple CAA based steering here to illustrate activation engineering method. \n",
    "\n",
    "* See the official [$k$-steering github repo](https://github.com/withmartian/nonlinear_steering) for $k$ steering, and see if you can get it working! Let me know if you run into difficulties, or submit a PR to make it more robust.\n",
    "\n",
    "* Can you come up with more examples of datasets and behaviors? For instance, [this paper]() introduces impatient, skeptical, confused and incoherent traits. And [this one]() introduces sycophancy, evil and hallucination. Finally, this paper adds the 5 basic emotions, namely joy, anger, fear, disgust and sadness.\n",
    "\n",
    "* Can you think of applications where you can inject behaviors into models like this?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llm_psychometrics)",
   "language": "python",
   "name": "llm_psychometrics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
